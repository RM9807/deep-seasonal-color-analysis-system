{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ulxDNcSNOq9",
        "outputId": "8c6b264a-2951-4237-f233-165005e2347e"
      },
      "outputs": [],
      "source": [
        "# === colab configuration ===\n",
        "# p.s. when training on colab, weights are saved on Drive (directory DSCAS/weights).\n",
        "# p.p.s. skip this cell if running demo file locally!\n",
        "\n",
        "! pip install torch-summary\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "# setting paths\n",
        "repository_path = '/content/deep-seasonal-color-analysis-system/'\n",
        "dataset_path = repository_path + 'headsegmentation_dataset_ccncsa/'\n",
        "dataset_path_drive = '/content/drive/MyDrive/DSCAS/headsegmentation_dataset_ccncsa/'\n",
        "weights_path = repository_path + 'models/weights/'\n",
        "weights_path_drive = '/content/drive/MyDrive/DSCAS/weights/'\n",
        "sys.path.insert(0, repository_path)\n",
        "\n",
        "# cloning project repository and downloading dataset\n",
        "drive.mount('/content/drive')\n",
        "! test ! -d $repository_path && git clone https://github.com/mrcmich/deep-seasonal-color-analysis-system.git\n",
        "! test ! -d $dataset_path && cp -R $dataset_path_drive $dataset_path\n",
        "%cd $repository_path\n",
        "\n",
        "# setting branch and pulling updates\n",
        "branch = 'main__fastscnn_hyperparameters_tuning'\n",
        "! git checkout $branch\n",
        "! git pull origin $branch\n",
        "\n",
        "executing_on_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUNWSumyhhyN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import ops\n",
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        "from models import dataset, training_and_testing\n",
        "from models.cloud.UNet import unet\n",
        "from metrics_and_losses import metrics\n",
        "from utils import segmentation_labels, utils\n",
        "import matplotlib.pyplot as plt\n",
        "from palette_classification import color_processing\n",
        "import torchsummary\n",
        "from models.config import *\n",
        "\n",
        "try:\n",
        "  executing_on_colab\n",
        "except NameError:\n",
        "  executing_on_colab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CRI0xDBhhyP"
      },
      "outputs": [],
      "source": [
        "# local configuration\n",
        "if executing_on_colab is False:\n",
        "  weights_path = 'models/weights/'\n",
        "  dataset_path = ROOT_DIR + 'headsegmentation_dataset_ccncsa/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSBN1RIFhhyP"
      },
      "outputs": [],
      "source": [
        "# defining transforms\n",
        "tH, tW = 256, 256\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] # from ImageNet\n",
        "image_transform = T.Compose([T.Resize((tH, tW)), T.Normalize(mean, std)])\n",
        "target_transform = T.Compose([T.Resize((tH, tW))])\n",
        "\n",
        "# fetching dataset\n",
        "n_classes = len(segmentation_labels.labels)\n",
        "img_paths, label_paths = dataset.get_paths(dataset_path, file_name='training.xml')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(img_paths, label_paths, test_size=0.20, random_state=99, shuffle=True)\n",
        "train_dataset = dataset.MyDataset(X_train, Y_train, image_transform, target_transform)\n",
        "test_dataset = dataset.MyDataset(X_test, Y_test, image_transform, target_transform)\n",
        "\n",
        "# setting up model and fixed (initially) hyperparameters\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "score_fn = metrics.batch_mIoU\n",
        "learning_rate = 0.01\n",
        "class_weights = torch.tensor([0.3762, 0.9946, 0.9974, 0.9855, 0.7569, 0.9140, 0.9968, 0.9936, 0.9989, 0.9893, 0.9968])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === selecting best loss function ===\n",
        "\n",
        "results = []\n",
        "\n",
        "loss_fn_candidates = [\n",
        "    nn.CrossEntropyLoss(),\n",
        "    nn.CrossEntropyLoss(weight=class_weights.to(device))]\n",
        "\n",
        "for loss_fn in loss_fn_candidates:\n",
        "    model = unet.UNet(out_channels=n_classes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    partial_results = training_and_testing.train_model(\n",
        "        device, model, train_dataset, batch_size, n_epochs, score_fn, loss_fn, optimizer, \n",
        "        lr_scheduler=None, verbose=True, evaluate=True)\n",
        "    results.append(partial_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross Entropy Loss\n",
        "last_train_score = results[0]['average_train_score'][-1]\n",
        "last_val_score = results[0]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[0], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weighted Cross Entropy Loss\n",
        "last_train_score = results[1]['average_train_score'][-1]\n",
        "last_val_score = results[1]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[1], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EREj36DuhhyQ",
        "outputId": "04201001-3405-460c-ab80-6b3198659652"
      },
      "outputs": [],
      "source": [
        "# === selecting best optimizer ===\n",
        "\n",
        "results = []\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "models = [\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "]\n",
        "\n",
        "optimizer_candidates = [\n",
        "    torch.optim.SGD(models[0].parameters(), lr=learning_rate),\n",
        "    torch.optim.Adam(models[1].parameters(), lr=learning_rate),\n",
        "    torch.optim.AdamW(models[2].parameters(), lr=learning_rate)]\n",
        "\n",
        "for i, optimizer in enumerate(optimizer_candidates):\n",
        "    model = models[i]\n",
        "    partial_results = training_and_testing.train_model(\n",
        "        device, model, train_dataset, batch_size, n_epochs, score_fn, loss_fn, optimizer, \n",
        "        lr_scheduler=None, verbose=True, evaluate=True)\n",
        "    results.append(partial_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SGD\n",
        "last_train_score = results[0]['average_train_score'][-1]\n",
        "last_val_score = results[0]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[0], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adam\n",
        "last_train_score = results[1]['average_train_score'][-1]\n",
        "last_val_score = results[1]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[1], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AdamW\n",
        "last_train_score = results[2]['average_train_score'][-1]\n",
        "last_val_score = results[2]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[2], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === selecting best LR scheduler ===\n",
        "\n",
        "models = [\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "    unet.UNet(out_channels=n_classes),\n",
        "]\n",
        "\n",
        "results = []\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizers = [\n",
        "    torch.optim.Adam(models[0].parameters(), lr=learning_rate),\n",
        "    torch.optim.Adam(models[1].parameters(), lr=learning_rate),\n",
        "    torch.optim.Adam(models[2].parameters(), lr=learning_rate)]\n",
        "\n",
        "scheduler_candidates = [\n",
        "    None,\n",
        "    torch.optim.lr_scheduler.LinearLR(optimizers[1], start_factor=0.05),\n",
        "    torch.optim.lr_scheduler.ExponentialLR(optimizers[2], gamma=0.05)]\n",
        "\n",
        "for i, scheduler in enumerate(scheduler_candidates):\n",
        "    model = models[i]\n",
        "    optimizer = optimizers[i]\n",
        "\n",
        "    partial_results = training_and_testing.train_model(\n",
        "        device, model, train_dataset, batch_size, n_epochs, score_fn, loss_fn, optimizer, \n",
        "        lr_scheduler=scheduler, verbose=True, evaluate=True)\n",
        "    results.append(partial_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# no scheduler\n",
        "last_train_score = results[0]['average_train_score'][-1]\n",
        "last_val_score = results[0]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[0], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# linear scheduler\n",
        "last_train_score = results[1]['average_train_score'][-1]\n",
        "last_val_score = results[1]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[1], plotsize=(20, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exponential scheduler\n",
        "last_train_score = results[2]['average_train_score'][-1]\n",
        "last_val_score = results[2]['average_val_score'][-1]\n",
        "print(f'training score after {n_epochs} epochs: {last_train_score}')\n",
        "print(f'validation score after {n_epochs} epochs: {last_val_score}')\n",
        "training_and_testing.plot_training_results(results[2], plotsize=(20, 6))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bff6c79cdac020ac98a972002b5dbe7766c5a0fccfcd9b44550d7582f6eef17b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
