{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from palette_classification import palette, color_processing\n",
    "from palette_classification.palettes import mappings\n",
    "from utils import utils\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import json\n",
    "from retrieval import clothes_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palettes_path = 'palette_classification/palettes/'\n",
    "dresscode_dataset_paths = {\n",
    "    'upper_body': 'dresscode_test_dataset/upper_body/',\n",
    "    'lower_body': 'dresscode_test_dataset/lower_body/',\n",
    "    'dresses': 'dresscode_test_dataset/dresses/',\n",
    "}\n",
    "\n",
    "# loading palettes\n",
    "palette_filenames = glob.glob(palettes_path + '*.csv')\n",
    "reference_palettes = [palette.PaletteRGB().load(\n",
    "    palette_filename.replace('\\\\', '/'), header=True) for palette_filename in palette_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference_palette in reference_palettes:\n",
    "  print(reference_palette.description())\n",
    "  reference_palette.plot(tile_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in dresscode_dataset_paths:\n",
    "    mapping_dict_for_category = {} # dict mapping between images and reference palettes\n",
    "    img_paths_for_category = glob.glob(dresscode_dataset_paths[category] + 'images/*.jpg')\n",
    "    n_images = len(img_paths_for_category)\n",
    "    print(f'\\nClassification of {category} images...')\n",
    "\n",
    "    for i, img_path in enumerate(img_paths_for_category):\n",
    "        img_path = img_path.replace('\\\\', '/')\n",
    "        print(f'|__Classification of image {i+1}/{n_images}...')\n",
    "\n",
    "        # computing masked and resized image\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        img = utils.from_HWD_to_DHW(torch.from_numpy(img))\n",
    "        _, H, W = img.shape\n",
    "        resize = T.Compose([T.Resize((H // 3, W // 3))])\n",
    "        mask = clothes_segmentation.segment_img_cloth(img_path)\n",
    "        mask_eroded = color_processing.erode_segmentation_mask(mask, kernel_size=50)\n",
    "        img_masked = color_processing.apply_masks(img, mask_eroded)\n",
    "        img_masked_resized = resize(img_masked)\n",
    "\n",
    "        # computing cloth embedding and assigning season palette\n",
    "        cloth_embedding = color_processing.compute_cloth_embedding(\n",
    "            img_masked_resized, max_length=8, ignored_colors=[[0, 0, 0]])\n",
    "        cloth_palette = palette.PaletteRGB('cloth', cloth_embedding)\n",
    "        cloth_season_palette = palette.classify_cloth_palette(\n",
    "            cloth_palette, reference_palettes, distance_type='avg')\n",
    "\n",
    "        mapping_dict_for_category[img_path] = mappings.DESC_ID_MAPPING[cloth_season_palette.description()]\n",
    "        \n",
    "    # saving mapping dict for category as json\n",
    "    with open(dresscode_dataset_paths[category] + 'palette_mappings.json', 'w') as outfile:\n",
    "        json.dump(mapping_dict_for_category, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dscas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8a8707fb11d5d89cf3731e99f3a893eb130d1c029541dff323dda234707d9bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
